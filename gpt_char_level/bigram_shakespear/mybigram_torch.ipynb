{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11ad035f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as file:\n",
    "\tdata = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dictionaries and Data loader Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is:  65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted({char for char in data})\n",
    "''.join(chars)\n",
    "\n",
    "ctoi = {c:i for i, c in enumerate(chars)}\n",
    "itoc = {i:c for c, i in ctoi.items()}\n",
    "vocab_size = len(chars)\n",
    "print(\"Vocab size is: \", vocab_size)\n",
    "\n",
    "encode = lambda x: [ctoi[i] for i in x]\n",
    "decode = lambda x: [itoc[i] for i in x]\n",
    "\n",
    "check_str = 'how do you do'\n",
    "\n",
    "assert check_str == ''.join(decode(encode(check_str))), \"Something is wrong with encode/decode function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = encode(data)\n",
    "\n",
    "data_split_index = int(len(encoded_data) * 0.9)\n",
    "train_data = torch.tensor(encoded_data[:data_split_index], dtype=torch.long)\n",
    "test_data = torch.tensor(encoded_data[data_split_index:], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([733043, 913292, 272072,  32495])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, len(train_data) - 5, size=(4, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(batch_size, context_size):\n",
    "\n",
    "\trand_indices = torch.randint(0, len(train_data) - context_size, size=(batch_size, ))\n",
    "\n",
    "\tx = torch.stack([train_data[ix:ix+context_size] for ix in rand_indices])\n",
    "\ty = torch.stack([train_data[ix+1:ix+context_size+1] for ix in rand_indices])\n",
    "\n",
    "\treturn x, y\n",
    "\n",
    "\n",
    "sample_x, sample_y = create_batch(4, 5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Class for Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape is: torch.Size([20, 65]), loss for sample data is: 4.511\n",
      "\n",
      "kQi'pK';ogOdzJnQLZ: \n",
      "L\n",
      "S;VAoLauZrMBwgI,cIsr&?ukct'.&wIVi&XcJXIA3k$JfxBVH:In:SQTZ,&I:NU\n",
      "HffkKFSBSBXy\n",
      "zCq?'.OnUAgqvwJ,OtUCxeVXLC'$q:IqRcEv-CbAz&xDqjdb-fl.\n",
      "SZv3Vkbbxe$B\n",
      "SVzf;BdzWfK:hMvdU LN ;q'iOEi,kcSPga!fVL\n",
      "NdV:qXaiz!RUVp HZpagIGxCiV\n",
      "i\n",
      "N'ABo U:sm,Iy?wBVVW?YDMbhXXny,3j\n",
      "i,cDDXOMrDU$zU\n",
      "wrwKuzPF3 Mdg'U'KXhZNtqRMZi\n",
      "!Bkn3e$mcESqF.;DFUk;IbCyX'tgiLk,'sn-nKhZOcj.H!cIj&w,yKfjTd?kHFrKQZN\n",
      "NhyIEwFcsEmcr,!KB; &e\n",
      "?&Jy!oYosJH-bJpFCZNKTQLgYsJPw:Nbej!bTt'CqPaQYqIaRvV3 tcUbMxHvsQC F\n",
      "?Pe$!qTxV:t3moN&AB,Ooheneo?Fo tj\n"
     ]
    }
   ],
   "source": [
    "class bigram(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, vocab_size, embedding_dim = 512):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.vocab_size = vocab_size\n",
    "\t\tself.embedding_dim = embedding_dim\n",
    "\t\tself.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\t\tself.dense = nn.Linear(self.embedding_dim, self.vocab_size)\n",
    "\n",
    "\tdef forward(self, x, targets = None):\n",
    "\t\t\n",
    "\t\tembedding_output = self.embedding(x) # shape Batch_size, context_size, embedding_dim\n",
    "\t\tlogits = self.dense(embedding_output) # batch_size, context_size, vocab_size\n",
    "\n",
    "\t\tif targets is None:\n",
    "\n",
    "\t\t\tloss = None\n",
    "\t\telse:\n",
    "\n",
    "\t\t\tbatch_size, context_size, vocab_size = logits.shape\n",
    "\n",
    "\t\t\tlogits = logits.view(batch_size * context_size, vocab_size)\n",
    "\n",
    "\t\t\ttargets = targets.view(batch_size * context_size)\n",
    "\n",
    "\t\t\tloss = F.cross_entropy(logits, targets)\n",
    "\n",
    "\t\treturn logits, loss\n",
    "\t\n",
    "\tdef generate(self, x, context_size):\n",
    "\n",
    "\t\tfor _ in range(context_size):\n",
    "\n",
    "\t\t\tembedding_output = self.embedding(x) # shape of embedding output is 1, <dynamic>, embedding_dimension\n",
    "\n",
    "\t\t\tlogits = self.dense(embedding_output) # shape of logits is 1, <dynamic>, vocab_size\n",
    "\n",
    "\t\t\tlogits = logits[:, -1, :] # shape B, vocab_size\n",
    "\n",
    "\t\t\tprobs = F.softmax(logits, dim=1)\n",
    "\n",
    "\t\t\tnext_sample = torch.multinomial(probs, 1) # batch-size, 1\n",
    "\n",
    "\t\t\tx = torch.concat((x, next_sample), dim=1)\n",
    "\n",
    "\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "bigram_model = bigram(vocab_size, 512)\n",
    "\n",
    "a, b = bigram_model(sample_x, sample_y)\n",
    "\n",
    "print(f\"Logits shape is: {a.shape}, loss for sample data is: {b.item():.3f}\")\n",
    "\n",
    "print(''.join(decode(bigram_model.generate(x=torch.zeros(1, 1, dtype=torch.long), context_size=500)[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch: 0, loss is: 4.301\n",
      "For epoch: 10, loss is: 3.244\n",
      "For epoch: 20, loss is: 2.930\n",
      "For epoch: 30, loss is: 2.666\n",
      "For epoch: 40, loss is: 2.634\n",
      "For epoch: 50, loss is: 2.575\n",
      "For epoch: 60, loss is: 2.644\n",
      "For epoch: 70, loss is: 2.510\n",
      "For epoch: 80, loss is: 2.496\n",
      "For epoch: 90, loss is: 2.549\n",
      "For epoch: 100, loss is: 2.520\n",
      "For epoch: 110, loss is: 2.487\n",
      "For epoch: 120, loss is: 2.490\n",
      "For epoch: 130, loss is: 2.506\n",
      "For epoch: 140, loss is: 2.527\n",
      "For epoch: 150, loss is: 2.526\n",
      "For epoch: 160, loss is: 2.473\n",
      "For epoch: 170, loss is: 2.454\n",
      "For epoch: 180, loss is: 2.568\n",
      "For epoch: 190, loss is: 2.516\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "batch_size = 100\n",
    "context_size = 10\n",
    "\n",
    "# creating optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(bigram_model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\ttrainx, trainy = create_batch(batch_size=batch_size, context_size=context_size)\n",
    "\n",
    "\n",
    "\tlogits, loss = bigram_model(trainx, trainy)\n",
    "\n",
    "\toptimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "\tloss.backward()\n",
    "\n",
    "\toptimizer.step()\n",
    "\n",
    "\tif epoch % 10 == 0:\n",
    "\t\tprint(f\"For epoch: {epoch}, loss is: {loss.item():.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wha myonghinday irerbbearorodeathistre kan of spiey plouth he chesed, s s f dwite frdee\n",
      "HAntis ceshiommarerwhuk t hig.\n",
      "Th m ay, prery.\n",
      "TERCor terlpe lers wiceng ferand carm at htheverdwhe a w k,\n",
      "LOUMwn parind s\n",
      "My, gos terthawo,\n",
      "FDUMIbua omouple thbandan tometlathelee Yound thitholedeans IO, anou heie we s.\n",
      "\n",
      "Becer ouk' meng you gooweexPrit ay otud l'dedeyou.\n",
      "aillou!\n",
      "PUCalinkne n wn wh fome; y hentour the bean d ous shesw ar.\n",
      "\n",
      "\n",
      "'shouson-thave efo rdomOoovethond qus; witr yougithak?\n",
      "sthe atie iom;\n"
     ]
    }
   ],
   "source": [
    "print(''.join(decode(bigram_model.generate(x=torch.zeros(1, 1, dtype=torch.long), context_size=500)[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
