{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(len(words))\n",
    "print(max(len(w) for w in words))\n",
    "print(words[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My aim here, is to create embedding for each of the characters, which I want to feed to RNN. I want to train the embeddings for each of the characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dictionaries for Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = sorted({char for word in words for char in word})\n",
    "charset = ['start'] + charset + ['.', '!']\n",
    "\n",
    "\n",
    "print(charset)\n",
    "\n",
    "itoc = {i:c for i, c in enumerate(charset)}\n",
    "ctoi = {c:i for i, c in itoc.items()}\n",
    "\n",
    "vocab_size = len(itoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sth = [len(w) for w in words]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `sth` is the list you mentioned\n",
    "plt.boxplot(sth, vert=False)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Variable')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Boxplot of sth')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting diff algorithm to build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 8\n",
    "def create_dataset(words, input_size = maxlen):\n",
    "\tX = []\n",
    "\tY = []\n",
    "\tsetup = [0] * 10\n",
    "\tfor word in words:\n",
    "\n",
    "\t\tfor ix in range(len(word)):\n",
    "\t\t\t\n",
    "\t\t\tlast_ix = ix - 1\n",
    "\t\t\tsetup[-(ix-1):] = word[:ix]\n",
    "\n",
    "\t\t\tX.append(setup)\n",
    "\t\t\tY.append(ctoi[word[ix]])\n",
    "\t\t\n",
    "\t\n",
    "\ttx = torch.tensor(X, dtype=torch.float32)\n",
    "\tty = torch.tensor(Y, dtype=torch.int32)\n",
    "\n",
    "\treturn tx, ty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1: this version of create_dataset, involves padding (from left, with 0) to have same maxlen no of characters in each sequence; tad bit inefficient\n",
    "\n",
    "* Time complexity will be O(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'christopher'\n",
    "#word = 'lucy'\n",
    "input_size = 8\n",
    "setup = [0] * input_size\n",
    "\n",
    "for ix in range(len(word)):\n",
    "\tlast_ix = ix - 1\n",
    "\tsetup[-(ix-1):] = word[:ix]\n",
    "\tprint(f\"For ix: {ix}, the setup is: {setup} and target is: {word[ix]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V2: This does it better...no padding required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'christopher'\n",
    "#word = 'lucy'\n",
    "input_size = maxlen\n",
    "setup = [0] * input_size\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "\n",
    "X.append([0] * maxlen)\n",
    "Y.append(word[0])\n",
    "for ix in range(1, len(word)):\n",
    "\t#print(f\"For ix: {ix}, pre-assign, setup: {setup[-last_ix:]}, word: {word[:ix]}\", end=\" \")\n",
    "\tcx = ix\n",
    "\tif ix > maxlen:\n",
    "\t\trem = ix % maxlen\n",
    "\n",
    "\t\tcx = ix - rem\n",
    "\t\tsetup[-cx:] = word[rem:ix]\n",
    "\telse:\n",
    "\t\tsetup[-cx:] = word[:ix]\n",
    "\n",
    "\tX.append(setup.copy())\n",
    "\tY.append(word[ix])\n",
    "\t#print(f\"For ix: {ix}, cx: {cx}, the setup is: {setup}, len of setup: {len(setup)},  and target is: {word[ix]}\")\n",
    "\n",
    "for x, y in zip(X, Y):\n",
    "\tprint(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(words, input_size = maxlen):\n",
    "\tX = []\n",
    "\tY = []\n",
    "\t\n",
    "\tfor word in words:\n",
    "\t\tprint(\"Processing word: \", word)\n",
    "\t\tsetup = [0] * maxlen\n",
    "\t\tX.append([0] * maxlen)\n",
    "\t\tY.append(ctoi[word[0]])\n",
    "\t\tfor ix in range(1, len(word)):\n",
    "\t\t\tcx = ix\n",
    "\t\t\tif ix > maxlen:\n",
    "\t\t\t\trem = ix % maxlen\n",
    "\n",
    "\t\t\t\tcx = ix - rem\n",
    "\t\t\t\tsetup[-cx:] = word[rem:ix]\n",
    "\t\t\telse:\n",
    "\t\t\t\tsetup[-cx:] = word[:ix]\n",
    "\n",
    "\t\t\tX.append(setup.copy())\n",
    "\t\t\tY.append(ctoi[word[ix]])\n",
    "\t\t\n",
    "\treturn X, Y\n",
    "\t#tx = torch.tensor(X, dtype=torch.float32)\n",
    "\t#ty = torch.tensor(Y, dtype=torch.int32)\n",
    "\n",
    "\t#return tx, ty\n",
    "\n",
    "#X, Y = create_dataset(words[:5])\n",
    "\n",
    "#for x, y in zip(X, Y):\n",
    "#\tprint(x, y)\n",
    "\n",
    "X, Y = create_dataset(words[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V3: Final version, based on cycling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'christopher'\n",
    "#word = 'lucy'\n",
    "input_size = maxlen\n",
    "setup = [0] * input_size\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for ix in range(0, len(word)):\n",
    "\n",
    "\n",
    "\tX.append(setup)\n",
    "\tY.append(word[ix])\n",
    "\n",
    "\tsetup = setup[1:] + [word[ix]]\n",
    "\t#print(f\"For ix: {ix}, the setup is: {setup}, len of setup: {len(setup)},  and target is: {word[ix]}\")\n",
    "\n",
    "#for x, y in zip(X, Y):\n",
    "#\tprint(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(words, input_size = maxlen):\n",
    "\tX = []\n",
    "\tY = []\n",
    "\t\n",
    "\tfor word in words:\n",
    "\t\tprint(\"Processing word: \", word)\n",
    "\t\tsetup = [0] * input_size\n",
    "\t\tX = []\n",
    "\t\tY = []\n",
    "\n",
    "\t\tfor ix in range(len(word)):\n",
    "\t\t\tX.append(setup)\n",
    "\t\t\tY.append(ctoi[word[ix]])\n",
    "\n",
    "\t\t\tsetup = setup[1:] + [ctoi[word[ix]]]\n",
    "\t\t\n",
    "\t#return X, Y\n",
    "\ttx = torch.tensor(X, dtype=torch.float32)\n",
    "\tty = torch.tensor(Y, dtype=torch.int32)\n",
    "\n",
    "\treturn tx, ty\n",
    "\n",
    "\n",
    "X, Y = create_dataset(words[:5])\n",
    "\n",
    "for x, y in zip(X, Y):\n",
    "\tprint(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(words, input_size = maxlen):\n",
    "\tX = []\n",
    "\tY = []\n",
    "\t\n",
    "\tfor word in words:\n",
    "\t\tword += '.'\n",
    "\t\t#print(\"Processing word: \", word)\n",
    "\t\tsetup = [0] * input_size\n",
    "\t\tfor ix in range(len(word)):\n",
    "\t\t\t#print(\"\\t, processing index: \", ix)\n",
    "\t\t\tX.append(setup)\n",
    "\t\t\tY.append(ctoi[word[ix]])\n",
    "\n",
    "\t\t\tsetup = setup[1:] + [ctoi[word[ix]]]\n",
    "\t\t\n",
    "\t#return X, Y\n",
    "\ttx = torch.tensor(X, dtype=torch.int32)\n",
    "\tty = torch.tensor(Y, dtype=torch.int32)\n",
    "\n",
    "\treturn tx, ty\n",
    "\n",
    "dx, dy = create_dataset(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Embedding Layer and feature/target tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_seed = 19234123\n",
    "embedding_dim = 100\n",
    "maxlen = 8 # given boxplot, 8 seems like a decent no\n",
    "\n",
    "gen = torch.Generator().manual_seed(generator_seed)\n",
    "embedding_matrix = torch.randn((vocab_size, embedding_dim), generator=gen, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tensor = embedding_matrix[dx]\n",
    "target_tensor = dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tensor = feature_tensor.to('mps')\n",
    "target_tensor = target_tensor.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Important check for feature_tensor, shape: {feature_tensor.shape}, dtype: {feature_tensor.dtype}, requires grad: {feature_tensor.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Important check for target_tensor, shape: {target_tensor.shape}, dtype: {target_tensor.dtype}, requires grad: {target_tensor.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving on to Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tensor.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Using nn.Module (keeping things tad bit simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Tanh, self).__init__()\n",
    "\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\treturn (torch.exp(2 * x) - 1) / (torch.exp(2 * x) + 1)\n",
    "\n",
    "\n",
    "\n",
    "class Softmax(nn.Module):\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Softmax, self).__init__()\n",
    "\n",
    "\tdef forward(self, logits):\n",
    "\n",
    "\t\tlogits_max = logits.max(dim=1, keepdim=True)\n",
    "\t\tlogits_norm = logits - logits_max[0]\n",
    "\n",
    "\t\texp_logits = logits_norm.exp()\n",
    "\t\texp_logits_sum = exp_logits.sum(1, keepdim=True)\n",
    "\n",
    "\t\treturn exp_logits/exp_logits_sum\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class rnn(nn.Module):\n",
    "\n",
    "\tdef __init__(self, embedding_dim, vocab_size = vocab_size, hidden_dim = 256):\n",
    "\t\tsuper(rnn, self).__init__()\n",
    "\n",
    "\t\tself.embedding_dim = embedding_dim\n",
    "\t\tself.hidden_dim = hidden_dim\n",
    "\t\tself.vocab_size = vocab_size\n",
    "\n",
    "\t\tself.tanh = Tanh()\n",
    "\t\tself.softmax = Softmax()\n",
    "\n",
    "\n",
    "\t\t#self.wxh = nn.Parameter(torch.randn(self.embedding_dim, self.hidden_dim))\n",
    "\t\tself.wxh = nn.Parameter(torch.nn.init.kaiming_normal_(torch.empty((self.embedding_dim, self.hidden_dim)), nonlinearity='linear'))\n",
    "\t\tself.bh = nn.Parameter(torch.randn(1, self.hidden_dim))\n",
    "\n",
    "\t\t#self.whh = nn.Parameter(torch.randn(self.hidden_dim, self.hidden_dim))\n",
    "\t\tself.whh = nn.Parameter(torch.nn.init.kaiming_normal_(torch.empty((self.hidden_dim, self.hidden_dim)), nonlinearity='linear'))\n",
    "\n",
    "\t\t#self.who = nn.Parameter(torch.randn(self.hidden_dim, self.vocab_size))\n",
    "\t\tself.who = nn.Parameter(torch.nn.init.kaiming_normal_(torch.empty((self.hidden_dim, self.vocab_size)), nonlinearity='linear'))\n",
    "\t\tself.bo = nn.Parameter(torch.randn(1, self.vocab_size))\n",
    "\n",
    "\tdef forward(self, x, hidden_state_prev=None):\n",
    "\n",
    "\t\tif hidden_state_prev is None:\n",
    "\t\t\thidden_state_prev = torch.zeros(x.shape[0], self.hidden_dim, device=x.device)\n",
    "\n",
    "\t\tinp_hid = x @ self.wxh + hidden_state_prev @ self.whh + self.bh\n",
    "\t\thidden_state = self.tanh(inp_hid)\n",
    "\n",
    "\t\tinp_out = hidden_state @ self.who + self.bo\n",
    "\t\toutput = self.softmax(inp_out)\n",
    "\n",
    "\t\treturn output, hidden_state\n",
    "\n",
    "\n",
    "\n",
    "def categorical_cce(y_pred, y_true):\n",
    "\tlog_probs = torch.log(y_pred[torch.arange(len(y_pred)), y_true])\n",
    "\tloss = -log_probs.mean()\n",
    "\treturn loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase 1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "myrnn = rnn(embedding_dim, hidden_dim=hidden_dim).to('mps')\n",
    "myrnn.train()\n",
    "\n",
    "myrnn.zero_grad()\n",
    "feature_tensor.device\n",
    "\n",
    "def grad_hook(grad):\n",
    "    print(\"Gradient at custom activation:\", grad)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\tx = embedding_matrix[dx].to('mps')\n",
    "\t\n",
    "\n",
    "\thidden_state_prev = torch.zeros(x.shape[0], hidden_dim, device=x.device)\n",
    "\thidden_state = hidden_state_prev\n",
    "\t\n",
    "\tfor t in range(maxlen):\n",
    "\t\toutput, hidden_state = myrnn(x[:, t, :], hidden_state.detach())\n",
    "\n",
    "\t\t#output.register_hook(grad_hook)\n",
    "\n",
    "\t\n",
    "\tloss = categorical_cce(output, target_tensor)\n",
    "\n",
    "\tloss.backward()\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor param in myrnn.parameters():\n",
    "\t\t\tparam.data = param.data - 0.01 * param.grad\n",
    "\t\n",
    "\tmyrnn.zero_grad()\n",
    "\n",
    "\tprint(f\"Epoch: {epoch}, loss: {loss:.3f}\")\n",
    "\t\n",
    "\t\t\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = torch.tensor([0] * 1)\n",
    "embedding_input = embedding_matrix[model_input].view(1, 100).to('mps')\n",
    "myrnn.eval()\n",
    "\n",
    "hidden_state = torch.zeros(1, hidden_dim, device='mps')\n",
    "pred = ''\n",
    "\n",
    "\n",
    "for names in range(10):\n",
    "\t\n",
    "\tfor _ in range(10):\n",
    "\n",
    "\t\toutput, hidden_state = myrnn(embedding_input, hidden_state)\n",
    "\t\t#argmax_output = torch.argmax(output, dim=1).to('cpu').item()\n",
    "\t\targmax_output = torch.multinomial(output, num_samples=1).to('cpu').item()\n",
    "\n",
    "\t\tpred += itoc[argmax_output]\n",
    "\t\tif argmax_output == 27:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tmodel_input = torch.tensor([argmax_output])\n",
    "\t\tembedding_input = embedding_matrix[model_input].view(1, 100).to('mps')\n",
    "\n",
    "\tmodel_input = torch.tensor([0] * 1)\n",
    "\tembedding_input = embedding_matrix[model_input].view(1, 100).to('mps')\n",
    "\n",
    "\tprint(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([103290, 103290, 391373, 391373, 408119, 408119, 469466, 469466, 488411, 488411, 535362, 535362, 538556, 538556, 600205, 600205, 609986, 609986, 636717, 636717, 651283, 651283, 689223, 689223, 696804, 696804, 699343, 699343, 728975, 728975, 792127, 792127, 794462, 794462, 814484, 814484, 821085, 821085, 828007, 828007, 833408, 833408, 840495, 840495, 2674440, 2674440, 2674750, 2674750, 2679101, 2679101, 2679164, 2679164, 2690521, 2690521, 2696644, 2696644, 2700803, 2700803, 2707116, 2707116, 2710708, 2710708, 2757225, 2757225, 2935358, 2935358]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
