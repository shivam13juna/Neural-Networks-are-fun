{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b394ffbb",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b52a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640b019",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "# Beginning the Construction of GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ab6d652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc126ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "\tno_of_layers: int = 12\n",
    "\tembedding_dim: int = 768\n",
    "\tvocab_size: int = 50257\n",
    "\tblock_size: int = 1000\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.config = config\n",
    "\n",
    "\t\tself.transformer = nn.ModuleDict({\n",
    "\t\t\t'wte' : nn.Embedding(config.vocab_size, config.embedding_dim),\n",
    "\t\t\t'wpe' : nn.Embedding(config.block_size, config.embedding_dim),\n",
    "\t\t\t'h' : nn.ModuleList([Block(config)])\n",
    "\n",
    "\t\t})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "448cd07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Raw Attention Scores ===\n",
      "tensor([[0.4928, 0.5976, 0.5355, 0.5677, 0.3879, 0.3832],\n",
      "        [0.6862, 0.5043, 0.5703, 0.6302, 0.3045, 0.3339],\n",
      "        [0.7786, 0.5263, 0.5879, 0.6733, 0.3143, 0.3676],\n",
      "        [0.7321, 0.6448, 0.6475, 0.7295, 0.4387, 0.4442],\n",
      "        [0.6297, 0.6150, 0.4805, 0.6117, 0.4627, 0.4856],\n",
      "        [0.4735, 0.2725, 0.3169, 0.3923, 0.1927, 0.2157]])\n",
      "\n",
      "=== After Applying Causal Mask ===\n",
      "tensor([[0.4928,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.6862, 0.5043,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.7786, 0.5263, 0.5879,   -inf,   -inf,   -inf],\n",
      "        [0.7321, 0.6448, 0.6475, 0.7295,   -inf,   -inf],\n",
      "        [0.6297, 0.6150, 0.4805, 0.6117, 0.4627,   -inf],\n",
      "        [0.4735, 0.2725, 0.3169, 0.3923, 0.1927, 0.2157]])\n",
      "\n",
      "=== After Applying Padding Mask ===\n",
      "tensor([[0.4928,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.6862, 0.5043,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.7786, 0.5263, 0.5879,   -inf,   -inf,   -inf],\n",
      "        [0.7321, 0.6448, 0.6475, 0.7295,   -inf,   -inf],\n",
      "        [0.6297, 0.6150, 0.4805, 0.6117, 0.4627,   -inf],\n",
      "        [0.4735, 0.2725, 0.3169, 0.3923, 0.1927,   -inf]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Setup dummy example\n",
    "B = 1  # batch size\n",
    "T = 6  # sequence length (max)\n",
    "d_k = 4  # key/query dim (small for demo)\n",
    "n_head = 1  # number of heads\n",
    "\n",
    "# Dummy query and key matrices (values don't matter, just shape)\n",
    "Q = torch.rand(B, n_head, T, d_k)\n",
    "K = torch.rand(B, n_head, T, d_k)\n",
    "\n",
    "# Compute raw attention scores\n",
    "attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "print(\"=== Raw Attention Scores ===\")\n",
    "print(attn_scores[0, 0])\n",
    "\n",
    "# Build causal mask [1, 1, T, T]\n",
    "causal_mask = torch.tril(torch.ones(T, T)).view(1, 1, T, T)\n",
    "attn_scores_causal = attn_scores.masked_fill(causal_mask == 0, float('-inf'))\n",
    "print(\"\\n=== After Applying Causal Mask ===\")\n",
    "print(attn_scores_causal[0, 0])\n",
    "\n",
    "# Build padding mask [B, 1, 1, T]\n",
    "# Let's simulate a sequence where last token is <pad>\n",
    "padding_mask = torch.tensor([[1, 1, 1, 1, 1, 0]], dtype=torch.float32).view(B, 1, 1, T)\n",
    "attn_scores_causal_padding = attn_scores_causal.masked_fill(padding_mask == 0, float('-inf'))\n",
    "print(\"\\n=== After Applying Padding Mask ===\")\n",
    "print(attn_scores_causal_padding[0, 0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
