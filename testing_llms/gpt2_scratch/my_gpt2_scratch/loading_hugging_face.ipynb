{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b9fd20",
   "metadata": {},
   "source": [
    "# Digging deeper into weights, GPT-2 huggingface transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c669511f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823bf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "sd_hf = model_hf.state_dict() \n",
    "\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "\tprint(k, v.shape)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5ddb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating total number of parameters\n",
    "total_params = 0\n",
    "for k, v in sd_hf.items():\n",
    "\ttotal_params += v.numel()\n",
    "print(f\"Total number of parameters in millions: {total_params / 1e6:.2f}\")\n",
    "print(f\"Total number of parameters in billions: {total_params / 1e9:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a08f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_hf['transformer.h.0.ln_1.weight'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dab99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(sd_hf[\"transformer.wpe.weight\"], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_hf[\"transformer.wpe.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 0])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 1])\n",
    "plt.plot(sd_hf[\"transformer.wpe.weight\"][:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc316823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "generator = pipeline('text-generation', model='gpt2-xl')\n",
    "set_seed(42)\n",
    "generated_texts = generator(\n",
    "\t\"\",\n",
    "\tmax_length=100,\n",
    "\tnum_return_sequences=5\n",
    ")\n",
    "\n",
    "for i, output in enumerate(generated_texts, start=1):\n",
    "\tprint(f\"Sequence {i}:\\n{output['generated_text']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36752de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline, set_seed\n",
    "\n",
    "# pick the MPS device if available, else fall back to CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# load model+tokenizer, move model to mps\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a84441c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9c5b3",
   "metadata": {},
   "source": [
    "# Synthesizing/Generating text with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7619f22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivam13juna/Documents/virtual_envs/appy/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/shivam13juna/Documents/virtual_envs/appy/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model device: mps:0\n",
      "single forward pass: 0.196s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline, set_seed\n",
    "\n",
    "# pick the MPS device if available, else fall back to CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# load model+tokenizer, move model to mps\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-xl\").to(device)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "# print where the model lives\n",
    "print(\"model device:\", next(model.parameters()).device)\n",
    "\n",
    "# run a quick timing check\n",
    "inp = tokenizer(\"Hello world\", return_tensors=\"pt\").to(model.device)\n",
    "start = time.time()\n",
    "_ = model(**inp)\n",
    "print(f\"single forward pass: {time.time() - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c2a674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1:\n",
      "Why do countries go to war? A common reason is that someone's family or friends have been killed. If someone's family is killed, they are more motivated to send troops into the war. For many countries that war has been caused by external aggression, e.g. in Iraq or Libya. The U.S. and Britain have launched wars on other countries to force a change of regime to that country to support the American interests. There were many other reasons (like money), but let's\n",
      "\n",
      "Sequence 2:\n",
      "Why do countries go to war? It's a question I asked in the 1970s and the answer was a military answer. Now it's more of an economic question,\" says Smejkal, co-author of The War We Never Fought: How the US Waged the Korean War, Vietnam, and Iraq.\n",
      "\n",
      "Smejkal is the author of three books on World War II in Korea, the subject of a Pulitzer Prize. In his book he traces America's\n",
      "\n",
      "Sequence 3:\n",
      "Why do countries go to war? Who really benefits or suffers? Is it just a matter of the cost, or is there more involved — some of them are very, very nasty countries. And I think this film, which is also very well done — so you can understand who the villains are.\n",
      "\n",
      "This film, called \"The Battle for Our Minds\"), is based on an interview Mr. Greenwald and Snowden gave to German daily Der Spiegel in 2013. It was the first interview with them\n",
      "\n",
      "Sequence 4:\n",
      "Why do countries go to war? Are there any good wars?\n",
      "\n",
      "A study of the wars that have occurred throughout history found that a surprising percentage of them involve the use of \"human shields.\" As I write this article, South Sudan is being accused of using its people as human shields to protect their government in the Sudanese civil war. We all know what \"human shields\" are — people who get sucked into the fighting and die there. However, this is a war the US didn't\n",
      "\n",
      "Sequence 5:\n",
      "Why do countries go to war? A history of war in the post-war era.\n",
      "\n",
      "World-wide, one of the main goals of war planning has been to use war as a means to achieve other political goals. In the post-World War II period, several new political objectives have emerged at the forefront of planning: the destruction of the \"evil empire\" of Japan, the reunification of Germany, and the stabilization of the \"evil empire\" of Russia. (In the later\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build your pipeline using the already‑moved model\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device='mps'               # None tells HF to respect model.device\n",
    ")\n",
    "\n",
    "generated_texts = generator(\n",
    "\t\"Why do countries go to war?\",\n",
    "\tmax_length=100,\n",
    "\tnum_return_sequences=5\n",
    ")\n",
    "\n",
    "for i, output in enumerate(generated_texts, start=1):\n",
    "\tprint(f\"Sequence {i}:\\n{output['generated_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77105bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# print where the model lives\n",
    "print(\"model device:\", next(model.parameters()).device)\n",
    "\n",
    "# run a quick timing check\n",
    "inp = tokenizer(\"Hello world\", return_tensors=\"pt\").to(model.device)\n",
    "start = time.time()\n",
    "_ = model(**inp)\n",
    "print(f\"single forward pass: {time.time() - start:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a08e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.backends.mps.is_available(), \"MPS not available on this machine\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "appy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
